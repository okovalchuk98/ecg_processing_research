{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import wfdb\n",
    "from wfdb import rdrecord\n",
    "import datetime\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from CustomStatisticCalculation import *\n",
    "from SubModeles.EcgDataset import ECGDataSetItem\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample\n",
    "from array_gzip_io_utils import save_array_as_gz_file\n",
    "from ecg_classification_helpers import generate_knowlage_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PART_SIZE = 8000\n",
    "EXPERIMENT = \"1\"\n",
    "KNOWLEDGE_WINDOWS_SIZE = 260\n",
    "new_sampling_rate = 400\n",
    "TEST_SET_SIZE = 0.2\n",
    "\n",
    "ROOT_DESTINATION_PATH = f\"TempData/Data/{PART_SIZE}/Test-set-{TEST_SET_SIZE}/{EXPERIMENT}\"\n",
    "ROOT_DATABASES_PATH = \"..\\..\\ECG\\Databases\"\n",
    "os.makedirs(ROOT_DESTINATION_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_time(index, fs):\n",
    "    time_ms = index * (1000 / fs)\n",
    "\n",
    "    time = datetime.datetime.utcfromtimestamp(time_ms/1000.0)\n",
    "    time_string = time.strftime('%H:%M:%S.%f')\n",
    "    return time_string\n",
    "\n",
    "def resample_ecg(signal, indexes, original_sampling, target_sampling):\n",
    "    resampling_factor = target_sampling / original_sampling\n",
    "    new_peak_indexes =np.asarray((indexes * resampling_factor).astype(int))\n",
    "    \n",
    "    new_length = int(np.ceil(len(signal) * resampling_factor))\n",
    "    resampled_signal = resample(signal, new_length)\n",
    "    return resampled_signal, new_peak_indexes\n",
    "\n",
    "def split_signal_to_dataset_items(signal, signal_name, annotation_indexes, annotation_names, supported_annotations, sampling_rate):\n",
    "    dataset_items = []\n",
    "    if not np.all(np.isfinite(signal)):\n",
    "        return dataset_items\n",
    "\n",
    "    if sampling_rate != new_sampling_rate:\n",
    "        signal, annotation_indexes = resample_ecg(signal, annotation_indexes, sampling_rate, new_sampling_rate)\n",
    "\n",
    "    for index in range(0, len(signal), PART_SIZE):\n",
    "        normalized_signal = minmax_scale(np.array(signal[index:index+PART_SIZE],dtype=np.float32))\n",
    "        if len(normalized_signal) < PART_SIZE:\n",
    "            continue\n",
    "        \n",
    "        part_targets = [0] * PART_SIZE\n",
    "        part_origin_r_peack = []\n",
    "        part_annotation_labels = []\n",
    "        part_annotation_indexs = np.where(np.logical_and(annotation_indexes>=index, annotation_indexes < PART_SIZE+index))\n",
    "        for annotation_index in part_annotation_indexs[0]:\n",
    "            annotation_symbol = annotation_names[annotation_index]\n",
    "            if annotation_symbol not in supported_annotations:\n",
    "                continue\n",
    "\n",
    "            part_annotation_labels.append(annotation_symbol)\n",
    "\n",
    "            part_targets[annotation_indexes[annotation_index] - index] = 1\n",
    "            part_origin_r_peack.append(annotation_indexes[annotation_index] - index)\n",
    "\n",
    "        part_origin_r_peack = np.array(part_origin_r_peack)\n",
    "        if not part_origin_r_peack.any():\n",
    "            continue\n",
    "\n",
    "        knowledge_part = generate_knowlage_integration(normalized_signal, KNOWLEDGE_WINDOWS_SIZE)\n",
    "        signal_start_time = get_index_time(index, new_sampling_rate)\n",
    "        dataset_items.append(ECGDataSetItem(normalized_signal, np.array(part_targets, dtype=np.float32), knowledge_part, part_origin_r_peack, np.array(part_annotation_labels), sampling_rate, signal_name, signal_start_time))\n",
    "\n",
    "    return dataset_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT arrhythmia dataset load\n",
    "class_mapper = { 'N': 0, 'V': 1, '/': 2, 'R': 3, 'L': 4, 'A': 5, '!': 6, 'E': 7, 'j': 8, 'a': 9, 'f': 10, 'F': 11, 'r': 12, 'J': 12, 'S': 12}\n",
    "\n",
    "signals_to_skip = [\"108\", \"207\"]\n",
    "LEAD_PRIORITIES = [\"MLII\", \"II\", \"V2\"]\n",
    "\n",
    "mit_db_path = os.path.join(ROOT_DATABASES_PATH, \"MIT\\mit-bih-arrhythmia-database-1.0.0\")\n",
    "\n",
    "dataset_items = []\n",
    "knowledge_window_number = math.ceil(PART_SIZE/KNOWLEDGE_WINDOWS_SIZE)\n",
    "with open(os.path.join(mit_db_path, \"RECORDS\")) as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0 or line in signals_to_skip:\n",
    "            continue\n",
    "\n",
    "        signal_path =os.path.join(mit_db_path, line)\n",
    "        record = rdrecord(signal_path)\n",
    "        ann = wfdb.rdann(signal_path, extension=\"atr\")\n",
    "        sampling_rate = record.fs\n",
    "\n",
    "        lead_index = 0\n",
    "        sig_name = record.sig_name[lead_index]\n",
    "        signal = record.p_signal.T[lead_index]\n",
    "\n",
    "        for lead in LEAD_PRIORITIES:\n",
    "            if lead in record.sig_name:\n",
    "                lead_index = record.sig_name.index(lead)\n",
    "                sig_name = record.sig_name[lead_index]\n",
    "                signal = record.p_signal.T[lead_index]\n",
    "\n",
    "                new_dataset_items = split_signal_to_dataset_items(signal, record.record_name, ann.sample, ann.symbol, class_mapper, sampling_rate)\n",
    "                dataset_items.extend(new_dataset_items)\n",
    "                break\n",
    "\n",
    "np.random.shuffle(dataset_items)\n",
    "train_data, test_data = train_test_split(dataset_items, test_size=TEST_SET_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Train set length: {len(train_data)}\")\n",
    "print(f\"Test set length: {len(test_data)}\")\n",
    "\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/train-mit-arrhythmia-fs-{new_sampling_rate}-prefered-leads.pkl.gz', train_data)\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/test-mit-arrhythmia-fs-{new_sampling_rate}-prefered-leads.pkl.gz', test_data)\n",
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QT dataset load\n",
    "supported_peak_annotation = ['A', 'S', 'V', 'F', 'a', 'Q', 'N', 'e', '/', 'j', 'R', 'J', 'f']\n",
    "\n",
    "database_folder = os.path.join(ROOT_DATABASES_PATH, \"qt-database-1.0.0\")\n",
    "with open(os.path.join(database_folder, \"RECORDS_Origin+lead\")) as file:\n",
    "    signal_settings =[]\n",
    "    for setting in file:\n",
    "        splited_settings = setting.split(\",\")\n",
    "        if len(splited_settings) == 3 and splited_settings[1]:\n",
    "            signal_settings.append([os.path.join(database_folder, splited_settings[0]), splited_settings[1],splited_settings[2].strip()])\n",
    "\n",
    "dataset_items = []\n",
    "all_annotations = []\n",
    "knowledge_window_number = math.ceil(PART_SIZE/KNOWLEDGE_WINDOWS_SIZE)\n",
    "for signal_setting in signal_settings:\n",
    "    signal_path, signal_lead, annotation_ext = signal_setting\n",
    "    record = rdrecord(signal_path)\n",
    "    sampling_rate = record.fs\n",
    "\n",
    "    if signal_lead in record.sig_name:\n",
    "        lead_index = record.sig_name.index(signal_lead)\n",
    "\n",
    "        # if os.path.exists(os.path.join(database_folder, f\"{record.record_name}.atr\")):\n",
    "        #     ann = wfdb.rdann(signal_path, extension=f\"atr\")\n",
    "        # else:\n",
    "        #     ann = wfdb.rdann(signal_path, extension=f\"pu{lead_index}\")\n",
    "\n",
    "        ann = wfdb.rdann(signal_path, extension=annotation_ext)\n",
    "        sig_name = record.sig_name[lead_index]\n",
    "        signal = record.p_signal.T[lead_index]\n",
    "\n",
    "        new_dataset_items = split_signal_to_dataset_items(signal, record.record_name, ann.sample, ann.symbol, supported_peak_annotation, sampling_rate)\n",
    "        dataset_items.extend(new_dataset_items)\n",
    "\n",
    "np.random.shuffle(dataset_items)\n",
    "\n",
    "train_data, test_data = train_test_split(dataset_items, test_size=TEST_SET_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Train set length: {len(train_data)}\")\n",
    "print(f\"Test set length: {len(test_data)}\")\n",
    "\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/train-qt-fs-{new_sampling_rate}-prefered-leads.pkl.gz', train_data)\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/test-qt-fs-{new_sampling_rate}-prefered-leads.pkl.gz', test_data)\n",
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChinaPhysiologicalSignalChallenge2020 dataset load\n",
    "knowledge_window_number = math.ceil(PART_SIZE/KNOWLEDGE_WINDOWS_SIZE)\n",
    "\n",
    "dataset_folder_path = os.path.join(ROOT_DATABASES_PATH, \"ChinaPhysiologicalSignalChallenge2020\")\n",
    "patient_number = 10\n",
    "sampling_rate = 400\n",
    "dataset_items = []\n",
    "\n",
    "for patient in range(1, patient_number + 1):\n",
    "    signal_file_path = os.path.join(dataset_folder_path, f\"A{str(patient).zfill(2)}.mat\")\n",
    "    annotation_file_path = os.path.join(dataset_folder_path, f\"RPN_{str(patient).zfill(2)}.mat\")\n",
    "\n",
    "    print(f\"Start processing ecg {patient}\", end=\"\\r\")\n",
    "    signal = loadmat(signal_file_path)\n",
    "    signal = np.asarray(signal['ecg'], dtype=np.float32)\n",
    "    signal = np.squeeze(signal)\n",
    "\n",
    "    ecg_annotation = loadmat(annotation_file_path)\n",
    "    ecg_annotation = np.asarray(ecg_annotation['R'], dtype=np.int32)\n",
    "    ecg_annotation = np.squeeze(ecg_annotation)\n",
    "\n",
    "    annotaiton_labels = ['R'] * len(ecg_annotation)\n",
    "\n",
    "    new_dataset_items = split_signal_to_dataset_items(signal, patient, ecg_annotation, annotaiton_labels, ['R'], sampling_rate)\n",
    "    dataset_items.extend(new_dataset_items)\n",
    "\n",
    "np.random.shuffle(dataset_items)\n",
    "\n",
    "train_data, test_data = train_test_split(dataset_items, test_size=TEST_SET_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/train-china-signal-chalenge-2020-fs-{new_sampling_rate}-prefered-leads.pkl.gz', train_data)\n",
    "del train_data\n",
    "\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/test-china-signal-chalenge-2020-fs-{new_sampling_rate}-prefered-leads.pkl.gz', test_data)\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland\n",
    "\"\"\"\n",
    "(C) Luis Howell <2123374H@student.gla.ac.uk>\n",
    "(C) 2018 Bernd Porr <bernd.porr@glasgow.ac.uk>\n",
    "\n",
    "GNU GENERAL PUBLIC LICENSE\n",
    "Version 3, 29 June 2007\n",
    "\n",
    "API for the data which loads, filters and exports\n",
    "the ECG data.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "# Class which loads the dataset\n",
    "class Ecg:\n",
    "\n",
    "    experiments = [\"sitting\",\"maths\",\"walking\"]\n",
    "    #experiments = [\"sitting\",\"maths\",\"walking\",\"hand_bike\",\"jogging\"]\n",
    "    fs=250\n",
    "    total_subjects = 25\n",
    "    \n",
    "    def __init__(self,root_dir,_subj,_experiment):\n",
    "        \"\"\" Constructor: Specify the path to the data, the subject number and the experiment\"\"\"\n",
    "        self.subj = _subj\n",
    "        self.experiment = _experiment\n",
    "        self.subjdir = root_dir+\"/\"+(\"subject_%02d\" % _subj)+\"/\"\n",
    "        self.expdir = self.subjdir+self.experiment+\"/\"\n",
    "\n",
    "        self.data=np.loadtxt(self.expdir+\"ECG.tsv\")\n",
    "        try:\n",
    "            self.anno_cs=np.loadtxt(self.expdir+\"annotation_cs.tsv\", dtype=int)\n",
    "            self.anno_cs_exists=True \n",
    "        except:\n",
    "            self.anno_cs_exists=False           \n",
    "        try:\n",
    "            self.anno_cables=np.loadtxt(self.expdir+\"annotation_cables.tsv\", dtype=int)\n",
    "            self.anno_cables_exists=True \n",
    "        except:\n",
    "            self.anno_cables_exists=False   \n",
    "           \n",
    "        self.cs_V2_V1 = self.data[:, 0]\n",
    "        self.einthoven_II = self.data[:, 1]\n",
    "        self.einthoven_III = self.data[:, 2]\n",
    "        self.einthoven_I = self.einthoven_II - self.einthoven_III\n",
    "        self.acc_x = self.data[:, 3]\n",
    "        self.acc_y = self.data[:, 4]\n",
    "        self.acc_z = self.data[:, 5]\n",
    "\n",
    "        self.T=1/self.fs\n",
    "        self.t = np.linspace(0, self.T*len(self.cs_V2_V1), len(self.cs_V2_V1))\n",
    "\n",
    "\n",
    "    def filter_data(self):\n",
    "        \"\"\"Filters the ECG data with a highpass at 0.1Hz and a bandstop around 50Hz (+/-2 Hz)\"\"\"\n",
    "\n",
    "        b_dc, a_dc = signal.butter(4, (0.1/self.fs*2), btype='highpass')\n",
    "        b_50, a_50 = signal.butter(4, [(48/self.fs*2),(52/self.fs*2)], btype='stop')\n",
    "\n",
    "        self.cs_V2_V1_filt = signal.lfilter(b_dc, a_dc, self.cs_V2_V1)\n",
    "        self.cs_V2_V1_filt = signal.lfilter(b_50, a_50, self.cs_V2_V1_filt)\n",
    "\n",
    "        self.einthoven_II_filt = signal.lfilter(b_dc, a_dc, self.einthoven_II)\n",
    "        self.einthoven_II_filt = signal.lfilter(b_50, a_50, self.einthoven_II_filt)\n",
    "\n",
    "        self.einthoven_III_filt = signal.lfilter(b_dc, a_dc, self.einthoven_III)\n",
    "        self.einthoven_III_filt = signal.lfilter(b_50, a_50, self.einthoven_III_filt)\n",
    "\n",
    "        self.einthoven_I_filt = self.einthoven_II_filt-self.einthoven_III_filt\n",
    "\n",
    "        return\n",
    "\n",
    "sampling_rate = 250\n",
    "data_path = os.path.join(ROOT_DATABASES_PATH, \"ECG_University_of_Glasgow\\dataset_716\\experiment_data\")\n",
    "dataset_items = []\n",
    "knowledge_window_number = math.ceil(PART_SIZE/KNOWLEDGE_WINDOWS_SIZE)\n",
    "\n",
    "for subject_index in range(25):\n",
    "    print(f\"Processing subject {subject_index}\", end=\"\\r\")\n",
    "\n",
    "    for experiment in Ecg.experiments:\n",
    "        ecg_class = Ecg(data_path, subject_index, experiment)\n",
    "        ecg_signal = ecg_class.einthoven_II\n",
    "\n",
    "        if ecg_class.anno_cables_exists:\n",
    "            ecg_annotation = ecg_class.anno_cables\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        annotaiton_labels = ['R'] * len(ecg_annotation)\n",
    "\n",
    "        new_dataset_items = split_signal_to_dataset_items(ecg_signal, f\"{subject_index}-{experiment}\", ecg_annotation, annotaiton_labels, ['R'], sampling_rate)\n",
    "        dataset_items.extend(new_dataset_items)\n",
    "\n",
    "np.random.shuffle(dataset_items)\n",
    "save_array_as_gz_file(f'{ROOT_DESTINATION_PATH}/university-of-glasgow-2020-fs-{new_sampling_rate}-prefered-leads.pkl.gz', dataset_items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchLightForEcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
